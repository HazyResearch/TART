{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part #1: How to use TART for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(f'{os.path.dirname(os.path.dirname(os.getcwd()))}')\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "from tart.tart_modules import Tart\n",
    "from tart.registry import DATASET_REGISTRY\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tqdm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !! Running this notebook with a pre-trained TART head !!\n",
    "* Download [pre-trained TART Reasoning module](https://github.com/HazyResearch/TART/releases/download/reasoning_module/tart_heads.zip) (you will need this if you want to run the sample notebooks in `src/notebooks`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://github.com/HazyResearch/TART/releases/download/reasoning_module/tart_heads.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CUSTOMIZE AS NEEDED ####\n",
    "path_tart_weights = '/u/scr/nlp/data/ic-fluffy-head-k-2/3e9724ed-5a49-4070-9b7d-4209a30e2392'\n",
    "cache_dir = '/u/scr/nlp/data/neo/hub'\n",
    "path_tart_config = 'tart_conf.yaml' # if you are using the pre-trained module above, don't change this!\n",
    "data_dir_path = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #1: Set-up TART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_EMBED_MODEL = \"EleutherAI/gpt-neo-125m\"\n",
    "PATH_TO_TART_REASONING_HEAD = f\"{path_tart_weights}/model_24000.pt\"\n",
    "TART_CONFIG = yaml.load(open(path_tart_config, \"r\"), Loader=yaml.FullLoader)\n",
    "CACHE_DIR = cache_dir\n",
    "DOMAIN = \"text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Instantiate TART module ####\n",
    "t = Tart(\n",
    "    embed_model_name=BASE_EMBED_MODEL,\n",
    "    path_to_pretrained_head=PATH_TO_TART_REASONING_HEAD,\n",
    "    tart_head_config=TART_CONFIG,\n",
    "    domain=DOMAIN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load TartReasoningHead ####\n",
    "t._load_tart_head(PATH_TO_TART_REASONING_HEAD, TART_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embed model: EleutherAI/gpt-neo-125m ...\n"
     ]
    }
   ],
   "source": [
    "#### Set TART LLM embed model ####\n",
    "#### Note: we can use any LLM here! TART is LLM-agnostic ####\n",
    "\n",
    "BASE_EMBED_MODEL = \"EleutherAI/gpt-neo-125m\"\n",
    "t.set_embed_model(BASE_EMBED_MODEL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #2: Load in sample data...\n",
    "* For this purposes of this demo, we will use 64 in-context examples, and evaluate on 4 test samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RUN THE FOLLOWING CELLS ####\n",
    "DATASET_NAME = \"sms_spam\" \n",
    "TOTAL_IN_CONTEXT_EXAMPLES = 64\n",
    "seed = 42\n",
    "k_range = [TOTAL_IN_CONTEXT_EXAMPLES] # number of samples  to use as in-context examples\n",
    "max_eval_samples = 4 # number of samples to evaluate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sms_spam (/u/scr/nlp/data/neo/hub/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005829811096191406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9c33b4025849b7a7e2b4ee8b00b973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = DATASET_REGISTRY[DOMAIN][DATASET_NAME](\n",
    "    total_train_samples=TOTAL_IN_CONTEXT_EXAMPLES,\n",
    "    k_range=k_range,\n",
    "    seed=seed,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    max_eval_samples=max_eval_samples,\n",
    ")\n",
    "\n",
    "X_ice, y_ice, X_test, y_test = dataset.get_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-context example input: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "In-context example label: 1\n"
     ]
    }
   ],
   "source": [
    "### RUN THE FOLLOWING CELLS TO INSPECT DATA ####\n",
    "print(f\"In-context example input: {X_ice[-1].strip()}\")\n",
    "print(f\"In-context example label: {y_ice[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sample input: Ha! I wouldn't say that I just didn't read anything into way u seemed. I don't like 2 be judgemental....i save that for fridays in the pub!\n",
      "Test sample label: 0\n"
     ]
    }
   ],
   "source": [
    "### RUN THE FOLLOWING CELLS TO INSPECT DATA ####\n",
    "\n",
    "print(f\"Test sample input: {X_test[0].strip()}\")\n",
    "print(f\"Test sample label: {y_test[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #3: Evaluate!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Embed the in-context examples and test samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_ice_embed,\n",
    "    X_test_embed,\n",
    "    y_ice_embed,\n",
    "    y_test_embed\n",
    ") = t.embed_layer.embed(\n",
    "    X_test, X_ice, y_ice, y_test, k=k_range[0], seed=seed, text_threshold=1000\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Concatenate embeddings\n",
    "* Returns a sequence of embeddings which is the concatenation of the embeddings of in-context examples with test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_seqs = t._concatenate_inputs(X_ice_embed, y_ice_embed, X_test_embed, y_test_embed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Predict\n",
    "* Pass the concatenated sequence of embeddings from Step 2 to the TART reasoning module to generate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: SMS Spam Classification\n",
      "\n",
      "\n",
      "Input: Ha! I wouldn't say that I just didn't read anything into way u seemed. I don't like 2 be judgemental....i save that for fridays in the pub!\n",
      "Ground Truth Label: 0\n",
      "TART Predicted Label: 0\n",
      "\n",
      "\n",
      "Input: K go and sleep well. Take rest:-).\n",
      "Ground Truth Label: 0\n",
      "TART Predicted Label: 0\n",
      "\n",
      "\n",
      "Input: Your next amazing xxx PICSFREE1 video will be sent to you enjoy! If one vid is not enough for 2day text back the keyword PICSFREE1 to get the next video.\n",
      "Ground Truth Label: 1\n",
      "TART Predicted Label: 1\n",
      "\n",
      "\n",
      "Input: Had your mobile 11mths ? Update for FREE to Oranges latest colour camera mobiles & unlimited weekend calls. Call Mobile Upd8 on freefone 08000839402 or 2StopTx\n",
      "Ground Truth Label: 1\n",
      "TART Predicted Label: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task: SMS Spam Classification\\n\\n\")\n",
    "for i, eval_seq in enumerate(eval_seqs):\n",
    "    pred = t.predict(eval_seq)\n",
    "    \n",
    "    print(f\"Input: {X_test[i].strip()}\")\n",
    "    print(f\"Ground Truth Label: {int(y_test_embed[i])}\") \n",
    "    print(f\"TART Predicted Label: {pred}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tart",
   "language": "python",
   "name": "tart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
