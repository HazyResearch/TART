{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part #2: How to train a TART reasoning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(f'{os.path.dirname(os.path.dirname(os.getcwd()))}')\n",
    "import warnings\n",
    "\n",
    "from reasoning_module.samplers import get_data_sampler\n",
    "from reasoning_module.tasks import get_task_sampler\n",
    "from reasoning_module.models import TransformerModel   \n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tqdm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Overview\n",
    "The goal of this notebook is to familiarize users with the TART reasoning module training process. The notebook will be structured into two parts: (1) training data exploration and (2) model architecture and training protocol exploration.\n",
    "\n",
    "Below we provide a brief description on the TART reasoning module parameters.\n",
    "\n",
    "Data sampling parameters:\n",
    "\n",
    "* n_dims (int): dimension of input data\n",
    "* n_positions (int): Total number of (x,y) pairs to sample for each sequence.\n",
    "* batch_size (int): size of train batches\n",
    "* weight_multiplier (int): represents noise level of problem, parameterized by ($\\alpha$)\n",
    "* variable_noise (bool): whether to randomly sample $\\alpha$ on a per batch level. If set to true, $\\alpha$ is uniformly sampled from $[1, \\alpha]$.\n",
    "\n",
    "Reasoning module parameters:\n",
    "* n_positions (int): Total # of (x,y) pairs in the input sequence: max_seq_length = 258 * 2\n",
    "* n_layer(int): # number of transformer layers\n",
    "* n_head (int): # number of attention heads\n",
    "* n_embd (int):  # hidden dimension of the reasoning module\n",
    "\n",
    "\n",
    "For the purposes of this exploration, we will use the following parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN THIS CELL ###\n",
    "\n",
    "# Data sampling parameters\n",
    "n_dims = 16 # number of dimensions of the input data\n",
    "n_positions=258 # total number of (x,y) pairs\n",
    "batch_size = 32 # batch size\n",
    "data_type = \"gaussian\" # input data distribution\n",
    "training_task_type = \"probabilistic_logistic_regression\" # task type\n",
    "weight_multiplier = 5 # weight multiplier for the task\n",
    "variable_noise=False # whether to use variable noise for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN THIS CELL ###\n",
    "\n",
    "# Reasoning module parameters\n",
    "n_positions = 258 # Total # of (x,y) pairs in the input sequence: max_seq_length = 258 * 2\n",
    "n_layer=12 # number of transformer layers\n",
    "n_head=8 # number of attention heads\n",
    "n_embd=256 # hidden dimension of the reasoning module\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Training data overview\n",
    "Recall that the TART reasoning head is trained on sequences ($s_t$) of $(x,y)$ pairs sample from $d$-dimensional logstic regression functions. We will begin by inspecting one training sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate task and data samplers\n",
    "data_sampler = get_data_sampler(data_type, n_dims=n_dims)\n",
    "task_sampler = get_task_sampler(\n",
    "            training_task_type,\n",
    "            n_dims,\n",
    "            batch_size,\n",
    "            weight_multiplier=weight_multiplier,\n",
    "            variable_noise=variable_noise,\n",
    "            n_points=n_positions, \n",
    "    )\n",
    "task = task_sampler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample the x and y's of an arbitrary sequence, $s_t$, with 258 $(x,y)$ pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample xs and ys\n",
    "xs, _ = data_sampler.sample_xs(n_positions, batch_size, n_dims)\n",
    "ys, _ = task.evaluate(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor of X's: torch.Size([32, 258, 16])\n",
      "Shape of tensor of Y's: torch.Size([32, 258])\n"
     ]
    }
   ],
   "source": [
    "# print shapes of xs and ys\n",
    "print(f\"Shape of tensor of X's: {xs.shape}\")\n",
    "print(f\"Shape of tensor of Y's: {ys.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that dim 0 = batch_size, dim 1 = n_positions (or sequence length), and dim 2 is n_dims (or hidden dimension size). Y's will be converted to a one-hot vector of dim 16 before being passed to the model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Training protocol \n",
    "In this section, we will review the reasoning module architecture and training protocol. In this excercise, we will be using the transformer architecture for the reasoning module (in subsquent notebooks we will show how different architectures can be used --- i.e., [Hyena](https://arxiv.org/abs/2302.10866)).\n",
    "\n",
    "Let us begin by instantiating out reasoning module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate reasoning module\n",
    "reasoning_module = TransformerModel(\n",
    "    n_dims=n_dims,\n",
    "    n_positions=n_positions,\n",
    "    n_embd=n_embd,\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now construct our input sequence given our xs and ys above. Recall that ys still doesn't have a 3rd dimension and needs to be converted to a one hot! Moreover, we need to interleave our x's and y's into a single sequence --- i.e., $x_0$, $y_0$, $x_1$, $y_1$ ... $x_{258}$, $y_{258}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input sequence: torch.Size([32, 516, 16])\n"
     ]
    }
   ],
   "source": [
    "# construct input sequence\n",
    "input_sequence = reasoning_module._combine(xs, ys)\n",
    "print(f\"Shape of input sequence: {input_sequence.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our input sequence has dimensions (batch_size, 2 * n_positions, n_dims). We will now take a single step with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform single forward pass\n",
    "output = reasoning_module._step(input_sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the loss over the output. The reasoning module is trained by computing the loss over the predicted $y_i$'s in sequence $s_t$. A binary cross entropy loss is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7249126434326172\n"
     ]
    }
   ],
   "source": [
    "# compute loss!\n",
    "loss_func = task.get_training_metric()\n",
    "loss = loss_func(output, ys)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wohoo! You have performed one training step for a TART reasoning module! In a full training run, we would perform $n$ such steps. It is important to note that for each step $t$ in training, we sample a *different* logistic regression problem to construct our $s_t$.\n",
    "\n",
    "To conduct a full training run, we refer readers to the following file for a sample \n",
    " `src/reasoning_module/conf/tart_heads/reasoning_head_s258.yaml` to see a sample configuration file.\n",
    "\n",
    "Given such a configuration file, training can be performed using:\n",
    "\n",
    "```\n",
    "python src/reasoning_module/train.py --config src/reasoning_module/conf/tart_heads/reasoning_head_s258.yaml\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tart",
   "language": "python",
   "name": "tart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
